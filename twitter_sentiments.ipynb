{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Twitter Sentiment\n",
    "\n",
    "This is my practice working through a set of problems from the [intro to NLP course by Shivam Bansal](https://courses.analyticsvidhya.com/courses/Intro-to-NLP).\n",
    "\n",
    "The course provides an example table of data containing ~15,000 tweets and their assocaited meta data. First we want to find the most common words in tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   X                                               text  favorited  \\\n",
      "1  1  RT @rssurjewala: Critical question: Was PayTM ...      False   \n",
      "2  2  RT @Hemant_80: Did you vote on #Demonetization...      False   \n",
      "3  3  RT @roshankar: Former FinSec, RBI Dy Governor,...      False   \n",
      "4  4  RT @ANI_news: Gurugram (Haryana): Post office ...      False   \n",
      "5  5  RT @satishacharya: Reddy Wedding! @mail_today ...      False   \n",
      "\n",
      "   favoriteCount replyToSN              created  truncated  replyToSID  \\\n",
      "1              0       NaN  2016-11-23 18:40:30      False         NaN   \n",
      "2              0       NaN  2016-11-23 18:40:29      False         NaN   \n",
      "3              0       NaN  2016-11-23 18:40:03      False         NaN   \n",
      "4              0       NaN  2016-11-23 18:39:59      False         NaN   \n",
      "5              0       NaN  2016-11-23 18:39:39      False         NaN   \n",
      "\n",
      "             id  replyToUID  \\\n",
      "1  8.014957e+17         NaN   \n",
      "2  8.014957e+17         NaN   \n",
      "3  8.014955e+17         NaN   \n",
      "4  8.014955e+17         NaN   \n",
      "5  8.014954e+17         NaN   \n",
      "\n",
      "                                        statusSource       screenName  \\\n",
      "1  <a href=\"http://twitter.com/download/android\" ...  HASHTAGFARZIWAL   \n",
      "2  <a href=\"http://twitter.com/download/android\" ...   PRAMODKAUSHIK9   \n",
      "3  <a href=\"http://twitter.com/download/android\" ...  rahulja13034944   \n",
      "4  <a href=\"http://twitter.com/download/android\" ...        deeptiyvd   \n",
      "5  <a href=\"http://cpimharyana.com\" rel=\"nofollow...        CPIMBadli   \n",
      "\n",
      "   retweetCount  isRetweet  retweeted  \n",
      "1           331       True      False  \n",
      "2            66       True      False  \n",
      "3            12       True      False  \n",
      "4           338       True      False  \n",
      "5           120       True      False  \n"
     ]
    }
   ],
   "source": [
    "dF = pd.read_csv('data/tweets.csv', index_col=0, encoding='ISO-8859-1')\n",
    "\n",
    "print(dF.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_freqs(text, n_top = 20):\n",
    "    \"\"\"\n",
    "    Count the frequency of each word and return the most frequent\n",
    "    \n",
    "    Parameters: \n",
    "    -----------\n",
    "    text : pandas series object\n",
    "        Pandas series object (column) of tweet texts\n",
    "    n_top : int\n",
    "        the number of most frequent entries to be returned\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    count_freqs : pandas Series\n",
    "        A pandas series with the most frequent words and the \n",
    "        frequency in which they appear\n",
    "    \"\"\"\n",
    "    # stack and split tweets into individual words\n",
    "    stacked = np.hstack(text.str.split())\n",
    "    \n",
    "    count_freqs = pd.Series(stacked).value_counts()[:n_top]\n",
    "    \n",
    "    return count_freqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RT                 11053\n",
       "to                  7650\n",
       "is                  5152\n",
       "in                  4491\n",
       "the                 4331\n",
       "of                  4053\n",
       "#Demonetization     3253\n",
       "demonetization      3162\n",
       "on                  2751\n",
       "#demonetization     2474\n",
       "PM                  2384\n",
       "Modi                2379\n",
       "India               2243\n",
       "and                 2220\n",
       "a                   2180\n",
       "that                2168\n",
       "out                 1729\n",
       "for                 1672\n",
       "so                  1599\n",
       "had                 1598\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_word_freqs(dF.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these most frequent words don't help us to gauge sentiment on twitter. For instance, RT, to, is, in, the, of, etc. are common but carry very little meaning. We would like to ignore these. Additionally, some words like demonetization appear often but due to different formattings appear multiple times. These should be counted together to get the true count of occurrence. We will now do some text formatting to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_freqs(text, n_top=20):\n",
    "    \n",
    "    stripped = []\n",
    "    \n",
    "    for tweet in text:\n",
    "        # replace RT with nothing\n",
    "        tweet = re.sub(r'RT', ' ', tweet)\n",
    "        \n",
    "        # replace special characters with nothing\n",
    "        tweet = re.sub(r'[?!.;:,#@-]', ' ', tweet)\n",
    "        \n",
    "        # replace &amp with &\n",
    "        tweet = re.sub(r'&amp', '&', tweet)\n",
    "        \n",
    "        # convert to lowercase for consistency when counting\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        # split tweet into individual words and append to master list\n",
    "        stripped.append(tweet.split())\n",
    "    \n",
    "    stacked = np.hstack(stripped)\n",
    "    count_freqs = pd.Series(stacked).value_counts()[:n_top]\n",
    "    \n",
    "    return count_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "demonetization    14461\n",
       "to                 7725\n",
       "https              6514\n",
       "//t                6142\n",
       "is                 5370\n",
       "the                5127\n",
       "in                 4717\n",
       "of                 4110\n",
       "and                2907\n",
       "on                 2820\n",
       "modi               2761\n",
       "india              2759\n",
       "pm                 2732\n",
       "a                  2392\n",
       "that               2202\n",
       "out                1861\n",
       "so                 1768\n",
       "for                1732\n",
       "by                 1677\n",
       "who                1624\n",
       "dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_word_freqs(dF.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have filtered out some issues here, but now we need to focus on removing words that carry little meaning. We will try again, but this time removing so called stop words that are common and don't carry meaning, using the python package nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/pdrew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the stop words\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "# print top 20 stop words\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_freqs(text, n_top=20):\n",
    "    \n",
    "    stripped = []\n",
    "    \n",
    "    for tweet in text:\n",
    "        # replace RT with nothing\n",
    "        tweet = re.sub(r'RT', ' ', tweet)\n",
    "        \n",
    "        # replace special characters with nothing\n",
    "        tweet = re.sub(r'[?!.;:,#@-]', ' ', tweet)\n",
    "        \n",
    "        # replace &amp with &\n",
    "        tweet = re.sub(r'&amp', '&', tweet)\n",
    "        \n",
    "        # convert to lowercase for consistency when counting\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        # split tweet into individual words and append to master list\n",
    "        stripped.append(tweet.split())\n",
    "    \n",
    "    stacked = np.hstack(stripped)\n",
    "    count_freqs = pd.Series(stacked).value_counts()[:n_top]\n",
    "    \n",
    "    return count_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is still a work in progress!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
